\section{PMEM Device}\label{sec:Device Types / PMEM Device}

The virtio pmem is a fake persistent memory (NVDIMM) device
used to bypass the guest page cache and provide a virtio
based asynchronous flush mechanism. This avoids the need
of a separate page cache in guest and keeps page cache only
in the host. Under memory pressure, the host makes use of
effecient memory reclaim decisions for page cache pages
of all the guests. This helps to reduce the memory footprint
and fit more guests in the host system.

\subsection{Device ID}\label{sec:Device Types / PMEM Device / Device ID}
  25

\subsection{Virtqueues}\label{sec:Device Types / PMEM Device / Virtqueues}
\begin{description}
\item[0] req_vq
\end{description}

\subsection{Feature bits}\label{sec:Device Types / PMEM Device / Feature bits}

There are currently no feature bits defined for this device.

\subsection{Device configuration layout}\label{sec:Device Types / PMEM Device / Device configuration layout}

\begin{lstlisting}
struct virtio_pmem_config {
	le64 start;
	le64 size;
};
\end{lstlisting}

\field{start} contains the guest physical address of the start of the
physical address range to be hotplugged into the guest address space
using the pmem API.
\field{size} contains the length of this address range.

\subsection{Device Initialization}\label{sec:Device Types / PMEM Device / Device Initialization}

Device hotplugs physical memory to guest address space. Persistent memory device
is emulated with file backed memory at host side.

\begin{enumerate}
\item Guest vpmem start is read from \field{start}.
\item Guest vpmem end is read from \field{size}.
\end{enumerate}

\devicenormative{\subsubsection}{Device Initialization}{Device Types / PMEM Device / Device Initialization}

File backed memory SHOULD be memory mapped to guest address space with SHARED
memory mapping.

\subsection{Driver Initialization}\label{sec:Device Types / PMEM Driver / Driver Initialization}

Driver hotplugs the physical memory and registers associated
region with the pmem API. Also, configures a flush callback
function with the corresponding region.

\drivernormative{\subsubsection}{Driver Initialization: Filesystem direct access}{Device Types / PMEM Driver / Driver Initialization / Direct access}

Driver SHOULD enable filesystem direct access operations for
read/write on the device.

\drivernormative{\subsubsection}{Driver Initialization: Virtio flush}{Device Types / PMEM Driver / Driver Initialization / Virtio flush}

Driver SHOULD add implement a virtio based flush callback.
Driver SHOULD disable other FLUSH/SYNC mechanisms for the device
when virtio flush is configured.

\subsection{Driver Operations}\label{sec:Device Types / PMEM Driver / Driver Operation}
\drivernormative{\subsubsection}{Driver Operation: Virtqueue command}{Device Types / PMEM Driver / Driver Operation / Virtqueue command}

Driver SHOULD send VIRTIO_FLUSH command on request virtqueue,
blocks guest userspace process utill host completes fsync/flush.
Driver SHOULD handle multiple fsync requests on files present
on the device.

\subsection{Device Operations}\label{sec:Device Types / PMEM Driver / Device Operation}

\devicenormative{\subsubsection}{Device Operations}{Device Types / PMEM Device / Device Operation / Virtqueue flush}

Device SHOULD handle multiple flush requests simultaneously using
host filesystem fsync/flush call.

\devicenormative{\subsubsection}{Device operations}{Device Types / PMEM Device / Device Operation / Virtqueue return}

Device SHOULD return integer '0' for success and '-1' for failure.
These errors are converted to corresponding error codes by guest as
per architecture.

\subsection{Possible security implications}\label{sec:Device Types / PMEM Device / Possible Security Implications}

There could be potential security implications depending on how
memory mapped host backing file is used. By default device emulation
is done with SHARED mapping. There is a contract between guest and host
process to access same backing file for read/write operations.

If a malicious guest or host userspace map the same backing file,
attacking process can make use of known cache side channel attacks
to predict the current state of shared page cache page. If both
attacker and victim somehow execute same shared code after a
flush/evict call, with difference in execution timing attacker
could infer another guest local data or host data. Though this is
not easy and same challenges exist as with bare metal host system
when userspace share same backing file.

\subsection{Countermeasures}\label{sec:Device Types / PMEM Device / Possible Security Implications / Countermeasures}

\subsubsection{ With SHARED mapping}\label{sec:Device Types / PMEM Device / Possible Security Implications / Countermeasures / SHARED}

If device backing backing file is shared with multiple guests or host
processes, this may act as a metric for page cache side channel attack.
As a counter measure every guest should have its own(not shared with
another guest) SHARED backing file and gets populated a per host process
page cache pages.

\subsubsection{ With PRIVATE mapping}\label{sec:Device Types / PMEM Device / Possible Security Implications / Countermeasures / PRIVATE}
There maybe be chances of side channels attack with PRIVATE
memory mapping similar to SHARED with read-only shared mappings.
PRIVATE is not used for virtio pmem making this usecase
irrelevant.

\subsubsection{ Workload specific mapping}\label{sec:Device Types / PMEM Device / Possible Security Implications / Countermeasures / Workload}
For SHARED mapping, if workload is single application inside
guest and there is no risk with sharing of data between guests.
Guest sharing same backing file with SHARED mapping can be
used as a valid configuration.

\subsubsection{ Prevent cache eviction}\label{sec:Device Types / PMEM Device / Possible Security Implications / Countermeasures / Cache eviction}
Don't allow cache evict from guest filesystem trim/discard command
with virtio pmem. This rules out any possibility of evict-reload
page cache side channel attacks if backing disk is shared(SHARED)
with mutliple guests. Though if we use per device backing file with
shared mapping this countermeasure is not required.
